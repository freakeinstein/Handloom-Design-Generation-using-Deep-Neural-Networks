{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pisamifa/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py:482: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if issubdtype(ts, int):\n",
      "/home/pisamifa/anaconda3/lib/python3.6/site-packages/scipy/misc/pilutil.py:485: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif issubdtype(type(size), float):\n",
      "/home/pisamifa/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] [0/435] time: 0:00:47.977190, [d_loss: 0.244784, g_loss: 0.821211]\n",
      "[16] [1/435] time: 0:00:58.680112, [d_loss: 0.152257, g_loss: 1.050221]\n",
      "[16] [2/435] time: 0:01:08.163687, [d_loss: 0.170006, g_loss: 1.042357]\n",
      "[16] [3/435] time: 0:01:17.522893, [d_loss: 0.055963, g_loss: 1.024050]\n",
      "[16] [4/435] time: 0:01:27.047800, [d_loss: 0.125756, g_loss: 1.357282]\n",
      "[16] [5/435] time: 0:01:36.393678, [d_loss: 0.138142, g_loss: 1.105225]\n",
      "[16] [6/435] time: 0:01:45.594870, [d_loss: 0.076915, g_loss: 0.973359]\n",
      "[16] [7/435] time: 0:01:55.082215, [d_loss: 0.056366, g_loss: 0.835286]\n",
      "[16] [8/435] time: 0:02:04.478982, [d_loss: 0.096328, g_loss: 0.837910]\n",
      "[16] [9/435] time: 0:02:13.854483, [d_loss: 0.022641, g_loss: 0.917081]\n",
      "[16] [10/435] time: 0:02:23.250118, [d_loss: 0.027821, g_loss: 1.157805]\n",
      "[16] [11/435] time: 0:02:32.609037, [d_loss: 0.162122, g_loss: 1.029477]\n",
      "[16] [12/435] time: 0:02:42.044007, [d_loss: 0.083148, g_loss: 1.145879]\n",
      "[16] [13/435] time: 0:02:51.430064, [d_loss: 0.118210, g_loss: 1.006639]\n",
      "[16] [14/435] time: 0:03:00.854591, [d_loss: 0.045426, g_loss: 1.076033]\n",
      "[16] [15/435] time: 0:03:10.012942, [d_loss: 0.082112, g_loss: 1.113445]\n",
      "[16] [16/435] time: 0:03:19.307294, [d_loss: 0.095984, g_loss: 1.024310]\n",
      "[16] [17/435] time: 0:03:28.616987, [d_loss: 0.072540, g_loss: 0.875664]\n",
      "[16] [18/435] time: 0:03:37.774347, [d_loss: 0.110558, g_loss: 0.864183]\n",
      "[16] [19/435] time: 0:03:47.040576, [d_loss: 0.101126, g_loss: 0.965698]\n",
      "[16] [20/435] time: 0:03:56.402833, [d_loss: 0.030807, g_loss: 1.075224]\n",
      "[16] [21/435] time: 0:04:05.617930, [d_loss: 0.046776, g_loss: 0.892389]\n",
      "[16] [22/435] time: 0:04:15.077741, [d_loss: 0.088087, g_loss: 1.027833]\n",
      "[16] [23/435] time: 0:04:24.305589, [d_loss: 0.071401, g_loss: 0.938025]\n",
      "[16] [24/435] time: 0:04:33.405233, [d_loss: 0.060094, g_loss: 1.076501]\n",
      "[16] [25/435] time: 0:04:42.726323, [d_loss: 0.101866, g_loss: 0.910778]\n",
      "[16] [26/435] time: 0:04:52.014357, [d_loss: 0.142212, g_loss: 1.077535]\n",
      "[16] [27/435] time: 0:05:01.264900, [d_loss: 0.068238, g_loss: 1.040169]\n",
      "[16] [28/435] time: 0:05:10.493257, [d_loss: 0.109377, g_loss: 1.254424]\n",
      "[16] [29/435] time: 0:05:19.710622, [d_loss: 0.125908, g_loss: 0.938855]\n",
      "[16] [30/435] time: 0:05:28.965845, [d_loss: 0.044027, g_loss: 0.930524]\n",
      "[16] [31/435] time: 0:05:38.159303, [d_loss: 0.045454, g_loss: 1.006032]\n",
      "[16] [32/435] time: 0:05:47.413432, [d_loss: 0.068753, g_loss: 1.082650]\n",
      "[16] [33/435] time: 0:05:56.633513, [d_loss: 0.074050, g_loss: 0.981952]\n",
      "[16] [34/435] time: 0:06:05.895842, [d_loss: 0.100621, g_loss: 1.056023]\n",
      "[16] [35/435] time: 0:06:15.196789, [d_loss: 0.106362, g_loss: 0.978347]\n",
      "[16] [36/435] time: 0:06:24.385052, [d_loss: 0.073834, g_loss: 1.424369]\n",
      "[16] [37/435] time: 0:06:33.775184, [d_loss: 0.127800, g_loss: 0.766640]\n",
      "[16] [38/435] time: 0:06:43.228837, [d_loss: 0.055683, g_loss: 0.963408]\n",
      "[16] [39/435] time: 0:06:52.367895, [d_loss: 0.042358, g_loss: 0.900753]\n",
      "[16] [40/435] time: 0:07:01.634382, [d_loss: 0.045633, g_loss: 1.097079]\n",
      "[16] [41/435] time: 0:07:10.884600, [d_loss: 0.106548, g_loss: 1.361660]\n",
      "[16] [42/435] time: 0:07:20.164739, [d_loss: 0.062498, g_loss: 1.027910]\n",
      "[16] [43/435] time: 0:07:29.265640, [d_loss: 0.130142, g_loss: 1.027943]\n",
      "[16] [44/435] time: 0:07:38.575639, [d_loss: 0.137850, g_loss: 1.131776]\n",
      "[16] [45/435] time: 0:07:47.822031, [d_loss: 0.174534, g_loss: 1.184888]\n",
      "[16] [46/435] time: 0:07:57.091030, [d_loss: 0.122953, g_loss: 0.992755]\n",
      "[16] [47/435] time: 0:08:06.299715, [d_loss: 0.114656, g_loss: 0.843753]\n",
      "[16] [48/435] time: 0:08:15.545335, [d_loss: 0.029704, g_loss: 0.904912]\n",
      "[16] [49/435] time: 0:08:24.784724, [d_loss: 0.066677, g_loss: 1.324640]\n",
      "[16] [50/435] time: 0:08:34.016803, [d_loss: 0.140415, g_loss: 1.029017]\n",
      "[16] [51/435] time: 0:08:43.670790, [d_loss: 0.118475, g_loss: 1.928197]\n",
      "[16] [52/435] time: 0:08:52.866677, [d_loss: 0.688612, g_loss: 1.458902]\n",
      "[16] [53/435] time: 0:09:02.133960, [d_loss: 0.287036, g_loss: 1.593341]\n",
      "[16] [54/435] time: 0:09:11.217215, [d_loss: 0.531636, g_loss: 1.983516]\n",
      "[16] [55/435] time: 0:09:20.657398, [d_loss: 0.271249, g_loss: 1.313240]\n",
      "[16] [56/435] time: 0:09:30.031126, [d_loss: 0.123927, g_loss: 1.099365]\n",
      "[16] [57/435] time: 0:09:39.487620, [d_loss: 0.085916, g_loss: 1.221498]\n",
      "[16] [58/435] time: 0:09:48.759619, [d_loss: 0.087074, g_loss: 0.981825]\n",
      "[16] [59/435] time: 0:09:57.966755, [d_loss: 0.047090, g_loss: 1.195195]\n",
      "[16] [60/435] time: 0:10:07.255168, [d_loss: 0.184682, g_loss: 1.339466]\n",
      "[16] [61/435] time: 0:10:16.590087, [d_loss: 0.133288, g_loss: 1.027753]\n",
      "[16] [62/435] time: 0:10:25.780196, [d_loss: 0.104670, g_loss: 1.095636]\n",
      "[16] [63/435] time: 0:10:35.116374, [d_loss: 0.090387, g_loss: 1.040836]\n",
      "[16] [64/435] time: 0:10:44.418776, [d_loss: 0.107366, g_loss: 1.073092]\n",
      "[16] [65/435] time: 0:10:53.725149, [d_loss: 0.111122, g_loss: 1.026798]\n",
      "[16] [66/435] time: 0:11:02.980862, [d_loss: 0.069578, g_loss: 1.064634]\n",
      "[16] [67/435] time: 0:11:12.092675, [d_loss: 0.099382, g_loss: 0.887867]\n",
      "[16] [68/435] time: 0:11:21.357573, [d_loss: 0.036729, g_loss: 0.882364]\n",
      "[16] [69/435] time: 0:11:30.648536, [d_loss: 0.046357, g_loss: 0.842641]\n",
      "[16] [70/435] time: 0:11:39.881405, [d_loss: 0.042658, g_loss: 0.900034]\n",
      "[16] [71/435] time: 0:11:49.139823, [d_loss: 0.097198, g_loss: 1.039312]\n",
      "[16] [72/435] time: 0:11:58.365940, [d_loss: 0.112986, g_loss: 1.016618]\n",
      "[16] [73/435] time: 0:12:07.621491, [d_loss: 0.070803, g_loss: 1.119068]\n",
      "[16] [74/435] time: 0:12:16.846040, [d_loss: 0.052980, g_loss: 0.986317]\n",
      "[16] [75/435] time: 0:12:26.114998, [d_loss: 0.061864, g_loss: 1.055110]\n",
      "[16] [76/435] time: 0:12:35.433454, [d_loss: 0.044214, g_loss: 1.127176]\n",
      "[16] [77/435] time: 0:12:44.694472, [d_loss: 0.036016, g_loss: 1.001958]\n",
      "[16] [78/435] time: 0:12:53.886550, [d_loss: 0.083151, g_loss: 0.979868]\n",
      "[16] [79/435] time: 0:13:03.043344, [d_loss: 0.035017, g_loss: 0.837874]\n",
      "[16] [80/435] time: 0:13:12.370884, [d_loss: 0.066776, g_loss: 0.762101]\n",
      "[16] [81/435] time: 0:13:21.861839, [d_loss: 0.041600, g_loss: 0.923648]\n",
      "[16] [82/435] time: 0:13:31.164560, [d_loss: 0.036278, g_loss: 1.055062]\n",
      "[16] [83/435] time: 0:13:40.427350, [d_loss: 0.081697, g_loss: 1.032692]\n",
      "[16] [84/435] time: 0:13:49.639992, [d_loss: 0.070063, g_loss: 1.535548]\n",
      "[16] [85/435] time: 0:13:58.815766, [d_loss: 0.035875, g_loss: 1.078148]\n",
      "[16] [86/435] time: 0:14:08.140400, [d_loss: 0.033208, g_loss: 1.383249]\n",
      "[16] [87/435] time: 0:14:17.387086, [d_loss: 0.072551, g_loss: 0.836833]\n",
      "[16] [88/435] time: 0:14:26.623138, [d_loss: 0.033499, g_loss: 0.849563]\n",
      "[16] [89/435] time: 0:14:35.738396, [d_loss: 0.034589, g_loss: 0.977953]\n",
      "[16] [90/435] time: 0:14:45.066269, [d_loss: 0.099111, g_loss: 2.046600]\n",
      "[16] [91/435] time: 0:14:54.454349, [d_loss: 0.051524, g_loss: 1.529478]\n",
      "[16] [92/435] time: 0:15:03.850505, [d_loss: 0.155808, g_loss: 1.318154]\n",
      "[16] [93/435] time: 0:15:13.092604, [d_loss: 0.089801, g_loss: 1.076771]\n",
      "[16] [94/435] time: 0:15:22.436917, [d_loss: 0.094007, g_loss: 1.493231]\n",
      "[16] [95/435] time: 0:15:31.672015, [d_loss: 0.040225, g_loss: 1.181830]\n",
      "[16] [96/435] time: 0:15:40.946667, [d_loss: 0.046047, g_loss: 1.145242]\n",
      "[16] [97/435] time: 0:15:49.945960, [d_loss: 0.059542, g_loss: 1.098069]\n",
      "[16] [98/435] time: 0:15:59.056534, [d_loss: 0.054970, g_loss: 0.889919]\n",
      "[16] [99/435] time: 0:16:08.313883, [d_loss: 0.035279, g_loss: 0.971007]\n",
      "[16] [100/435] time: 0:16:17.561422, [d_loss: 0.060538, g_loss: 1.096182]\n",
      "[16] [101/435] time: 0:16:27.204035, [d_loss: 0.031863, g_loss: 0.933810]\n",
      "[16] [102/435] time: 0:16:36.495332, [d_loss: 0.026805, g_loss: 0.951397]\n",
      "[16] [103/435] time: 0:16:45.665134, [d_loss: 0.039201, g_loss: 1.442042]\n",
      "[16] [104/435] time: 0:16:54.807320, [d_loss: 0.011724, g_loss: 0.836634]\n",
      "[16] [105/435] time: 0:17:04.061185, [d_loss: 0.016435, g_loss: 1.068888]\n",
      "[16] [106/435] time: 0:17:13.399969, [d_loss: 0.021440, g_loss: 0.885874]\n",
      "[16] [107/435] time: 0:17:22.575778, [d_loss: 0.038193, g_loss: 1.002856]\n",
      "[16] [108/435] time: 0:17:31.876135, [d_loss: 0.019909, g_loss: 1.138436]\n",
      "[16] [109/435] time: 0:17:41.150342, [d_loss: 0.044632, g_loss: 1.095047]\n",
      "[16] [110/435] time: 0:17:50.473973, [d_loss: 0.028317, g_loss: 0.894534]\n",
      "[16] [111/435] time: 0:17:59.673143, [d_loss: 0.016075, g_loss: 0.949739]\n",
      "[16] [112/435] time: 0:18:08.894324, [d_loss: 0.012481, g_loss: 1.136303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] [113/435] time: 0:18:18.182783, [d_loss: 0.014027, g_loss: 1.060748]\n",
      "[16] [114/435] time: 0:18:27.467522, [d_loss: 0.019584, g_loss: 0.992877]\n",
      "[16] [115/435] time: 0:18:36.624652, [d_loss: 0.025039, g_loss: 1.189870]\n",
      "[16] [116/435] time: 0:18:45.781803, [d_loss: 0.027457, g_loss: 0.754089]\n",
      "[16] [117/435] time: 0:18:55.109750, [d_loss: 0.028017, g_loss: 1.021206]\n",
      "[16] [118/435] time: 0:19:04.371543, [d_loss: 0.025942, g_loss: 1.187881]\n",
      "[16] [119/435] time: 0:19:13.610505, [d_loss: 0.021541, g_loss: 1.002033]\n",
      "[16] [120/435] time: 0:19:22.917719, [d_loss: 0.047847, g_loss: 1.303359]\n",
      "[16] [121/435] time: 0:19:32.110601, [d_loss: 0.081367, g_loss: 1.179337]\n",
      "[16] [122/435] time: 0:19:41.365558, [d_loss: 0.038624, g_loss: 1.084696]\n",
      "[16] [123/435] time: 0:19:50.593163, [d_loss: 0.027862, g_loss: 1.089847]\n",
      "[16] [124/435] time: 0:19:59.802857, [d_loss: 0.029035, g_loss: 0.993284]\n",
      "[16] [125/435] time: 0:20:09.071295, [d_loss: 0.028007, g_loss: 1.097177]\n",
      "[16] [126/435] time: 0:20:18.281062, [d_loss: 0.052813, g_loss: 1.082577]\n",
      "[16] [127/435] time: 0:20:27.517786, [d_loss: 0.012357, g_loss: 0.800552]\n",
      "[16] [128/435] time: 0:20:36.741348, [d_loss: 0.034304, g_loss: 0.877429]\n",
      "[16] [129/435] time: 0:20:46.069032, [d_loss: 0.026801, g_loss: 0.875836]\n",
      "[16] [130/435] time: 0:20:55.371996, [d_loss: 0.072305, g_loss: 1.223898]\n",
      "[16] [131/435] time: 0:21:04.623999, [d_loss: 0.028609, g_loss: 0.953796]\n",
      "[16] [132/435] time: 0:21:13.788458, [d_loss: 0.026696, g_loss: 1.206507]\n",
      "[16] [133/435] time: 0:21:23.038181, [d_loss: 0.007790, g_loss: 1.184838]\n",
      "[16] [134/435] time: 0:21:32.285488, [d_loss: 0.019787, g_loss: 1.338324]\n",
      "[16] [135/435] time: 0:21:41.544648, [d_loss: 0.029511, g_loss: 1.099542]\n",
      "[16] [136/435] time: 0:21:50.782912, [d_loss: 0.021205, g_loss: 0.923805]\n",
      "[16] [137/435] time: 0:21:59.988923, [d_loss: 0.029402, g_loss: 1.089678]\n",
      "[16] [138/435] time: 0:22:09.243856, [d_loss: 0.037076, g_loss: 0.953398]\n",
      "[16] [139/435] time: 0:22:18.448461, [d_loss: 0.024837, g_loss: 1.115648]\n",
      "[16] [140/435] time: 0:22:27.778367, [d_loss: 0.013578, g_loss: 1.100411]\n",
      "[16] [141/435] time: 0:22:37.290922, [d_loss: 0.112226, g_loss: 1.008005]\n",
      "[16] [142/435] time: 0:22:46.705630, [d_loss: 0.033174, g_loss: 1.141046]\n",
      "[16] [143/435] time: 0:22:56.086876, [d_loss: 0.165115, g_loss: 1.108744]\n",
      "[16] [144/435] time: 0:23:05.363210, [d_loss: 0.032436, g_loss: 1.168756]\n",
      "[16] [145/435] time: 0:23:14.550919, [d_loss: 0.035098, g_loss: 0.924506]\n",
      "[16] [146/435] time: 0:23:23.741613, [d_loss: 0.030221, g_loss: 0.890968]\n",
      "[16] [147/435] time: 0:23:32.963385, [d_loss: 0.019464, g_loss: 1.112386]\n",
      "[16] [148/435] time: 0:23:42.132616, [d_loss: 0.037858, g_loss: 1.019304]\n",
      "[16] [149/435] time: 0:23:51.352292, [d_loss: 0.056287, g_loss: 1.232541]\n",
      "[16] [150/435] time: 0:24:00.514803, [d_loss: 0.033785, g_loss: 0.886983]\n",
      "[16] [151/435] time: 0:24:10.280336, [d_loss: 0.095478, g_loss: 1.035642]\n",
      "[16] [152/435] time: 0:24:19.602968, [d_loss: 0.075268, g_loss: 1.056695]\n",
      "[16] [153/435] time: 0:24:28.756850, [d_loss: 0.049327, g_loss: 0.993397]\n",
      "[16] [154/435] time: 0:24:38.042044, [d_loss: 0.078750, g_loss: 0.966486]\n",
      "[16] [155/435] time: 0:24:47.399441, [d_loss: 0.027204, g_loss: 1.068257]\n",
      "[16] [156/435] time: 0:24:56.703139, [d_loss: 0.021571, g_loss: 0.900759]\n",
      "[16] [157/435] time: 0:25:05.900463, [d_loss: 0.012956, g_loss: 0.822226]\n",
      "[16] [158/435] time: 0:25:15.132239, [d_loss: 0.021495, g_loss: 0.953381]\n",
      "[16] [159/435] time: 0:25:24.435306, [d_loss: 0.056260, g_loss: 0.993291]\n",
      "[16] [160/435] time: 0:25:33.739316, [d_loss: 0.023213, g_loss: 0.974082]\n",
      "[16] [161/435] time: 0:25:42.936861, [d_loss: 0.052109, g_loss: 1.079007]\n",
      "[16] [162/435] time: 0:25:52.089701, [d_loss: 0.020296, g_loss: 1.188285]\n",
      "[16] [163/435] time: 0:26:01.394028, [d_loss: 0.020997, g_loss: 0.770269]\n",
      "[16] [164/435] time: 0:26:10.651134, [d_loss: 0.016043, g_loss: 1.118235]\n",
      "[16] [165/435] time: 0:26:19.794889, [d_loss: 0.086097, g_loss: 1.290965]\n",
      "[16] [166/435] time: 0:26:29.171770, [d_loss: 0.090280, g_loss: 1.265014]\n",
      "[16] [167/435] time: 0:26:38.470029, [d_loss: 0.077900, g_loss: 1.024099]\n",
      "[16] [168/435] time: 0:26:47.718722, [d_loss: 0.036674, g_loss: 1.040030]\n",
      "[16] [169/435] time: 0:26:57.171766, [d_loss: 0.066190, g_loss: 1.101770]\n",
      "[16] [170/435] time: 0:27:06.488735, [d_loss: 0.038740, g_loss: 1.095571]\n",
      "[16] [171/435] time: 0:27:15.791392, [d_loss: 0.062809, g_loss: 1.005793]\n",
      "[16] [172/435] time: 0:27:25.033910, [d_loss: 0.029517, g_loss: 0.901244]\n",
      "[16] [173/435] time: 0:27:34.487275, [d_loss: 0.030981, g_loss: 1.046982]\n",
      "[16] [174/435] time: 0:27:43.854733, [d_loss: 0.069268, g_loss: 1.208666]\n",
      "[16] [175/435] time: 0:27:53.066143, [d_loss: 0.051536, g_loss: 0.861269]\n",
      "[16] [176/435] time: 0:28:02.286991, [d_loss: 0.026144, g_loss: 0.839820]\n",
      "[16] [177/435] time: 0:28:11.479360, [d_loss: 0.027416, g_loss: 1.007765]\n",
      "[16] [178/435] time: 0:28:20.766235, [d_loss: 0.047202, g_loss: 0.868745]\n",
      "[16] [179/435] time: 0:28:29.891372, [d_loss: 0.040570, g_loss: 0.934155]\n",
      "[16] [180/435] time: 0:28:39.174737, [d_loss: 0.044376, g_loss: 1.054248]\n",
      "[16] [181/435] time: 0:28:48.569823, [d_loss: 0.029602, g_loss: 0.774146]\n",
      "[16] [182/435] time: 0:28:57.856199, [d_loss: 0.021558, g_loss: 1.074657]\n",
      "[16] [183/435] time: 0:29:07.141867, [d_loss: 0.079007, g_loss: 1.070877]\n",
      "[16] [184/435] time: 0:29:16.397073, [d_loss: 0.064752, g_loss: 0.973262]\n",
      "[16] [185/435] time: 0:29:25.611513, [d_loss: 0.091436, g_loss: 1.240511]\n",
      "[16] [186/435] time: 0:29:34.800736, [d_loss: 0.120065, g_loss: 1.008280]\n",
      "[16] [187/435] time: 0:29:43.994368, [d_loss: 0.052730, g_loss: 0.920645]\n",
      "[16] [188/435] time: 0:29:53.341922, [d_loss: 0.116021, g_loss: 0.936530]\n",
      "[16] [189/435] time: 0:30:02.578549, [d_loss: 0.056357, g_loss: 1.283908]\n",
      "[16] [190/435] time: 0:30:11.706241, [d_loss: 0.094935, g_loss: 1.497940]\n",
      "[16] [191/435] time: 0:30:20.933185, [d_loss: 0.076521, g_loss: 0.870044]\n",
      "[16] [192/435] time: 0:30:30.173792, [d_loss: 0.023114, g_loss: 0.854622]\n",
      "[16] [193/435] time: 0:30:39.345028, [d_loss: 0.029062, g_loss: 0.988378]\n",
      "[16] [194/435] time: 0:30:48.492365, [d_loss: 0.100892, g_loss: 1.047318]\n",
      "[16] [195/435] time: 0:30:57.893971, [d_loss: 0.076032, g_loss: 0.886948]\n",
      "[16] [196/435] time: 0:31:07.090546, [d_loss: 0.068584, g_loss: 0.952368]\n",
      "[16] [197/435] time: 0:31:16.359965, [d_loss: 0.078738, g_loss: 1.192363]\n",
      "[16] [198/435] time: 0:31:25.705143, [d_loss: 0.030238, g_loss: 1.181344]\n",
      "[16] [199/435] time: 0:31:35.061893, [d_loss: 0.028514, g_loss: 0.786126]\n",
      "[16] [200/435] time: 0:31:44.391039, [d_loss: 0.026183, g_loss: 1.065831]\n",
      "[16] [201/435] time: 0:31:54.091976, [d_loss: 0.027136, g_loss: 0.888939]\n",
      "[16] [202/435] time: 0:32:03.265739, [d_loss: 0.014077, g_loss: 0.953020]\n",
      "[16] [203/435] time: 0:32:12.481901, [d_loss: 0.025080, g_loss: 0.904307]\n",
      "[16] [204/435] time: 0:32:21.758003, [d_loss: 0.024908, g_loss: 1.052175]\n",
      "[16] [205/435] time: 0:32:30.901654, [d_loss: 0.075427, g_loss: 1.130916]\n",
      "[16] [206/435] time: 0:32:40.284977, [d_loss: 0.026548, g_loss: 1.008947]\n",
      "[16] [207/435] time: 0:32:49.498265, [d_loss: 0.042541, g_loss: 1.035894]\n",
      "[16] [208/435] time: 0:32:58.702016, [d_loss: 0.042897, g_loss: 0.926744]\n",
      "[16] [209/435] time: 0:33:07.962276, [d_loss: 0.044540, g_loss: 0.789464]\n",
      "[16] [210/435] time: 0:33:17.207643, [d_loss: 0.044765, g_loss: 0.862288]\n",
      "[16] [211/435] time: 0:33:26.572208, [d_loss: 0.031926, g_loss: 1.144166]\n",
      "[16] [212/435] time: 0:33:35.907237, [d_loss: 0.039924, g_loss: 1.059272]\n",
      "[16] [213/435] time: 0:33:45.010021, [d_loss: 0.041709, g_loss: 1.020425]\n",
      "[16] [214/435] time: 0:33:54.351301, [d_loss: 0.038472, g_loss: 1.172262]\n",
      "[16] [215/435] time: 0:34:03.513631, [d_loss: 0.033450, g_loss: 1.235973]\n",
      "[16] [216/435] time: 0:34:12.780287, [d_loss: 0.056852, g_loss: 0.793993]\n",
      "[16] [217/435] time: 0:34:22.073475, [d_loss: 0.030589, g_loss: 1.217178]\n",
      "[16] [218/435] time: 0:34:31.508822, [d_loss: 0.046154, g_loss: 1.213750]\n",
      "[16] [219/435] time: 0:34:40.769736, [d_loss: 0.025154, g_loss: 0.886035]\n",
      "[16] [220/435] time: 0:34:50.066185, [d_loss: 0.033731, g_loss: 1.024423]\n",
      "[16] [221/435] time: 0:34:59.285605, [d_loss: 0.033553, g_loss: 1.131962]\n",
      "[16] [222/435] time: 0:35:08.548176, [d_loss: 0.047376, g_loss: 1.116832]\n",
      "[16] [223/435] time: 0:35:17.812814, [d_loss: 0.041951, g_loss: 1.009632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] [224/435] time: 0:35:27.160647, [d_loss: 0.048327, g_loss: 0.956988]\n",
      "[16] [225/435] time: 0:35:36.144907, [d_loss: 0.058790, g_loss: 0.919583]\n",
      "[16] [226/435] time: 0:35:45.461189, [d_loss: 0.059516, g_loss: 0.776746]\n",
      "[16] [227/435] time: 0:35:54.694307, [d_loss: 0.011918, g_loss: 0.840529]\n",
      "[16] [228/435] time: 0:36:03.888759, [d_loss: 0.014115, g_loss: 0.825762]\n",
      "[16] [229/435] time: 0:36:13.196127, [d_loss: 0.022120, g_loss: 0.863413]\n",
      "[16] [230/435] time: 0:36:22.580772, [d_loss: 0.011610, g_loss: 0.809491]\n",
      "[16] [231/435] time: 0:36:31.907656, [d_loss: 0.010837, g_loss: 0.856619]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.datasets import mnist\n",
    "#from keras_contrib.layers.normalization import Normalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from data_loader import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class DiscoGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'saree'\n",
    "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "                                      img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Objectives\n",
    "        # + Adversarial: Fool domain discriminators\n",
    "        # + Translation: Minimize MAE between e.g. fake B and true B\n",
    "        # + Cycle-consistency: Minimize MAE between reconstructed images and original\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        fake_B, fake_A,\n",
    "                                        reconstr_A, reconstr_B ])\n",
    "        self.combined.load_weights('saved_model/model15.h5')\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mae', 'mae'],\n",
    "                              optimizer=optimizer)\n",
    "        \n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, normalize=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.4)(d)\n",
    "            if normalize:\n",
    "                d = BatchNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, normalize=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1,\n",
    "                            padding='same', activation='tanh')(u7)\n",
    "        model=Model(d0, output_img)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.4)(d)\n",
    "            if normalization:\n",
    "                d = BatchNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(16,epochs):\n",
    "\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, valid, \\\n",
    "                                                                         imgs_B, imgs_A, \\\n",
    "                                                                         imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                # Plot the progress\n",
    "                print (\"[%d] [%d/%d] time: %s, [d_loss: %f, g_loss: %f]\" % (epoch, batch_i,\n",
    "                                                                        self.data_loader.n_batches,\n",
    "                                                                        elapsed_time,\n",
    "                                                                        d_loss[0], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                #if batch_i % sample_interval == 0:\n",
    "                #   self.sample_images(epoch, batch_i)\n",
    "            \n",
    "            # serialize weights to HDF5\n",
    "            self.g_AB.save(\"saved_model/actual_model\"+str(epoch)+\".h5\")\n",
    "            self.combined.save_weights(\"saved_model/model\"+str(epoch)+\".h5\")\n",
    "            print(\"Saved model \"+str(epoch)+\" to disk\")\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A, imgs_B = self.data_loader.load_data(batch_size=1, is_testing=True)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A)\n",
    "        fake_A = self.g_BA.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i))\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gan = DiscoGAN()\n",
    "    gan.train(epochs=18,batch_size=4, sample_interval=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
